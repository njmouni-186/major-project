{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample inputs:\n",
      "1 :  ChatGPT: Optimizing Language Models for Dialogue https://t.co/K9rKRygYyn @OpenAI\n",
      "2 :  Try talking with ChatGPT, our new AI system which is optimized for dialogue. Your feedback will help us improve it. https://t.co/sHDm57g3Kr\n",
      "3 :  ChatGPT: Optimizing Language Models for Dialogue https://t.co/GLEbMoKN6w #AI #MachineLearning #DataScience #ArtificialIntelligence\\n\\nTrending AI/ML Article Identified &amp; Digested via Granola; a Machine-Driven RSS Bot by Ramsey Elbasheer https://t.co/RprmAXUp34\n",
      "sample outputs:\n",
      "1 :  0\n",
      "2 :  1\n",
      "3 :  -1\n"
     ]
    }
   ],
   "source": [
    "#Loading the Dataset\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "data = pd.read_csv('gptfile.csv')\n",
    "\n",
    "#Pre-Prcoessing and Bag of Word Vectorization using Count Vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "cv = CountVectorizer(stop_words='english',ngram_range = (1,1),tokenizer = token.tokenize)\n",
    "text_counts = cv.fit_transform(data['Sentences'])\n",
    "#Splitting the data into trainig and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(text_counts, data['Sentiment'], test_size=0.25, random_state=5)\n",
    "#Training the model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "MNB = MultinomialNB()\n",
    "MNB.fit(X_train, Y_train)\n",
    "#Caluclating the accuracy score of the model\n",
    "from sklearn import metrics\n",
    "predicted = MNB.predict(X_test)\n",
    "#print(predicted)\n",
    "#accuracy_score = metrics.accuracy_score(predicted, Y_test)\n",
    "#print(\"Accuracuy Score: \",accuracy_score),average='micro'\n",
    "print(\"sample inputs:\")\n",
    "for i in range(3):\n",
    "    print(i+1,\": \",data['Sentences'][i])\n",
    "\n",
    "y=MNB.predict(text_counts)\n",
    "print(\"sample outputs:\")\n",
    "for i in range(3):\n",
    "    print(i+1,\": \",y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB Classifier\n",
      "accuracy : 0.73053772070626\n",
      "F1 score : 0.73053772070626\n",
      "Precision: 0.73053772070626\n",
      "Recall   : 0.73053772070626\n",
      "Cross validation score : \n",
      " [0.69931601 0.68937528 0.7000456  0.69475604 0.70257205]\n",
      "Mean of cross validation score :  0.6972129972102873\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "y_pred=MNB.predict(X_test)\n",
    "print('MultinomialNB Classifier')\n",
    "print('accuracy :',accuracy_score(Y_test,y_pred))\n",
    "print('F1 score :',f1_score(Y_test,y_pred,average='micro'))\n",
    "print('Precision:',precision_score(Y_test,y_pred,average='micro'))\n",
    "print('Recall   :',recall_score(Y_test,y_pred,average='micro'))\n",
    "print('Cross validation score : \\n',cross_val_score(estimator = MNB, X = X_test, y = Y_test, cv =5))\n",
    "print('Mean of cross validation score : ',cross_val_score(estimator = MNB, X = X_test, y = Y_test, cv =5).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'teacher', \"Ma'am\", 'Maam', 'she']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kiran\\AppData\\Local\\Temp\\ipykernel_21180\\54967986.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[i] = sample\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]]\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " inputs (InputLayer)         [(None, 200)]             0         \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 200, 50)           5000000   \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 64)                29440     \n",
      "                                                                 \n",
      " FC1 (Dense)                 (None, 1024)              66560     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 1024)              0         \n",
      "                                                                 \n",
      " out_layer (Dense)           (None, 3)                 3075      \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 3)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,099,075\n",
      "Trainable params: 5,099,075\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "2/2 [==============================] - 5s 989ms/step - loss: 1.0826 - accuracy: 0.4105 - val_loss: 0.9336 - val_accuracy: 0.7066\n",
      "Epoch 2/20\n",
      "2/2 [==============================] - 1s 540ms/step - loss: 0.8872 - accuracy: 0.7353 - val_loss: 1.0730 - val_accuracy: 0.7066\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "import pandas as pd\n",
    "from textblob import Word\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "import nltk\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding, Bidirectional\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "#Loading the dataset\n",
    "data = pd.read_csv('finalDataset0.2.csv')\n",
    "X=data[\"Sentences\"]\n",
    "Y=data[\"Sentiment\"]\n",
    "all_stopwords = stopwords.words('english')\n",
    "all_stopwords.append(\"teacher\")\n",
    "all_stopwords.append(\"Ma'am\")\n",
    "all_stopwords.append(\"Maam\")\n",
    "all_stopwords.append(\"she\")\n",
    "\n",
    "print(all_stopwords)\n",
    "for i in range(0,X.shape[0]):\n",
    "    sample = X[i]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    sample = re.sub(\"[^a-zA-Z']\", ' ', sample)\n",
    "    sample = re.sub(r\"can\\'t\",'can not',sample)\n",
    "    sample = re.sub(r\"n\\'t\", \" not\", sample)\n",
    "    sample = sample.lower()\n",
    "    sample = sample.split()\n",
    "    sample = [lemmatizer.lemmatize(word) for word in sample if not word in set(all_stopwords)]\n",
    "    sample = ' '.join(sample)\n",
    "    X[i] = sample\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(Y)\n",
    "dummy_y = to_categorical(Y)\n",
    "print(dummy_y)\n",
    "Y=dummy_y\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=5)\n",
    "#print(X.head())\n",
    "#print(X.shape)\n",
    "#print(Y.shape)\n",
    "\n",
    "\n",
    "max_words = 100000\n",
    "max_len = 200\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(X_train.values)\n",
    "sequences = tok.texts_to_sequences(X_train.values)\n",
    "sequences_matrix = pad_sequences(sequences,maxlen=max_len)\n",
    "#Y_train = np.array(Y_train)\n",
    "#Y_test = np.array(Y_test) \n",
    "#Model Building\n",
    "#'''\n",
    "def RNN():\n",
    "    inputs = Input(name='inputs',shape=[max_len])\n",
    "    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n",
    "    layer = LSTM(64)(layer)\n",
    "    layer = Dense(1024,name='FC1')(layer)\n",
    "    layer = Dropout(0.5)(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dense(3,name='out_layer')(layer)\n",
    "    layer = Activation('softmax')(layer)\n",
    "    model = Model(inputs=inputs,outputs=layer)\n",
    "    return model\n",
    "\n",
    "model = RNN()\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])\n",
    "\n",
    "#'''\n",
    "'''\n",
    "    layer = Dropout(0.25)(layer)    \n",
    "    layer = Dense(256,name='FC2')(layer)\n",
    "    layer = Activation('softmax')(layer)\n",
    "    \n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(10, return_sequences=True),input_shape=(5, 10)))\n",
    "model.add(Bidirectional(LSTM(10)))\n",
    "model.add(Dense(5))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "'''\n",
    "# With custom backward layer\n",
    "'''model = Sequential()\n",
    "forward_layer = LSTM(10, return_sequences=True)\n",
    "backward_layer = LSTM(10, activation='relu', return_sequences=True,go_backwards=True)\n",
    "model.add(Bidirectional(forward_layer, backward_layer=backward_layer,input_shape=(5, 10)))\n",
    "model.add(Dense(5))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "'''\n",
    "\n",
    "#Model Training\n",
    "history=model.fit(sequences_matrix,Y_train,batch_size=512,epochs=20,validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.00000001)])\n",
    "#Model Testing\n",
    "test_sequences = tok.texts_to_sequences(X_test)\n",
    "test_sequences_matrix = pad_sequences(test_sequences,maxlen=max_len)\n",
    "#accr = model.evaluate(test_sequences_matrix,Y_test)\n",
    "#print(accr)\n",
    "#print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 1s 30ms/step\n",
      "[0.03456594 0.0493983  0.9160357 ]\n",
      "0.99999994\n",
      "[[  0   0  37]\n",
      " [  0   0  35]\n",
      " [  0   0 206]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        37\n",
      "           1       0.00      0.00      0.00        35\n",
      "           2       0.74      1.00      0.85       206\n",
      "\n",
      "    accuracy                           0.74       278\n",
      "   macro avg       0.25      0.33      0.28       278\n",
      "weighted avg       0.55      0.74      0.63       278\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kiran\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\kiran\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\kiran\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "preds = model.predict(test_sequences_matrix) \n",
    "print(preds[0]) \n",
    "print(np.sum(preds[0])) \n",
    "matrix = confusion_matrix(Y_test.argmax(axis=1), preds.argmax(axis=1))\n",
    "print(matrix)\n",
    "# more detail on how well things were predicted\n",
    "print(classification_report(Y_test.argmax(axis=1), preds.argmax(axis=1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample inputs:\n",
      "1 :  punctual also give u practical knowledge theortical\n",
      "2 :  good\n",
      "3 :  excellent lecture delivered teacher teacher punctual\n",
      "4 :  good\n",
      "5 :  teacher give u information required improve performance\n",
      "6 :  yes\n",
      "7 :  good punctual\n",
      "8 :  good\n",
      "9 :  good\n",
      "10 :  good\n",
      "26/26 [==============================] - 1s 33ms/step\n",
      "sample outputs:\n",
      "1 :  1\n",
      "2 :  1\n",
      "3 :  1\n",
      "4 :  1\n",
      "5 :  1\n",
      "6 :  1\n",
      "7 :  1\n",
      "8 :  1\n",
      "9 :  1\n",
      "10 :  1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"sample inputs:\")\n",
    "for i in range(10):\n",
    "    print(i+1,\": \",data['Sentences'][i])\n",
    "\n",
    "max_words = 100000\n",
    "max_len = 200\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(X.values)\n",
    "tsequences = tok.texts_to_sequences(X.values)\n",
    "tsequences_matrix = pad_sequences(sequences,maxlen=max_len)\n",
    "\n",
    "\n",
    "y=model.predict(tsequences_matrix)\n",
    "print(\"sample outputs:\")\n",
    "for i in range(10):\n",
    "    print(i+1,\": \",y.argmax(axis=1)[i]-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cdccfa0eaa626dc999dad3eb08a374748be4fc6da2dbd968f87331012c6faadd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
